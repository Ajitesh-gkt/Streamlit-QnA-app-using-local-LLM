This is a QnA app hosted on streamlit. The app works on locally downloaded LLM model mistral-7b-instruct-v0.1.Q4_K_S.gguf
The model can be downloaded at https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF.
The model runs on CPU resoruces and is a hard requirement for the LLM chains (Other models can be used too if you have good specifications of the device)
